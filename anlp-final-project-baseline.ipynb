{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9874117,"sourceType":"datasetVersion","datasetId":6061843},{"sourceId":9886880,"sourceType":"datasetVersion","datasetId":6071561},{"sourceId":164394,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":139845,"modelId":162464}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dependency install","metadata":{}},{"cell_type":"code","source":"!pip install datasets\n!pip install -q -U google-generativeai","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import re\nimport os\nimport time\nimport nltk\nimport wandb\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom torch.utils import data\nfrom collections import Counter\nfrom google.api_core import retry\nimport google.generativeai as gemini_ai\nfrom transformers import GPT2TokenizerFast\nfrom kaggle_secrets import UserSecretsClient\nfrom google.generativeai.types import RequestOptions\nfrom datasets import load_dataset, Dataset, DatasetDict\nfrom torch.nn import TransformerDecoder, TransformerEncoder\nfrom torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerFast\nfrom tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"## API Keys","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Wandb","metadata":{}},{"cell_type":"code","source":"wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.login(key=wandb_api_key)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Gemini","metadata":{}},{"cell_type":"code","source":"gemini_api_key = user_secrets.get_secret(\"GEMINI_API_KEY\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gemini_ai.configure(api_key=gemini_api_key)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Running on GPU (if available)","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Config","metadata":{}},{"cell_type":"markdown","source":"## Config for Hyperparameters","metadata":{}},{"cell_type":"code","source":"config = {\n    \"BLOCK_SIZE\": 128,\n    \"EMB_SIZE\": 768,\n    \"N_ATTENTION_HEADS\": 4,\n    \"N_DECODER_BLOCKS\": 2,\n    \"VOCAB_SIZE\": 10000,\n    \"MAX_OUT_TOKENS\": 200,\n    \"EVAL_ITER\": 100,\n    \"LR\": 3e-4,\n    \"BATCH_SIZE\": 32,\n    \"EPOCHS\": 5,\n    \"PATIENCE\": 3,\n    \"MODEL_NAME\": \"baseline-21\",\n    \"WORKING_DIR\": \"/kaggle/working\",\n    \"VOCAB_DIRNAME\": \"/kaggle/input/vocab-dict-v2/vocab_dict_v2\",\n    \"LOAD_MODELPATH\": \"/kaggle/input/baseline-21m/pytorch/default/1/baseline-21.pt\",\n    \"DF_PATH\": \"/kaggle/input/compare-dataframes/rating_df_baseline-21.pkl\",\n    \"DEVICE\": 'cuda' if torch.cuda.is_available() else 'cpu'\n}\nassert config['EMB_SIZE'] % config['N_ATTENTION_HEADS'] == 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.init(\n    project='baseline-21M',\n    config=config\n)\ntext_table = wandb.Table(columns=['epoch', 'loss', 'predicted text'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Config for training","metadata":{}},{"cell_type":"code","source":"load_model = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"load_df = True","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset","metadata":{}},{"cell_type":"markdown","source":"## Download the dataset","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"roneneldan/TinyStories\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Build the Vocabulary","metadata":{}},{"cell_type":"code","source":"# def preprocess_text(text):\n#     # Split text into sentences\n#     sentences = nltk.sent_tokenize(text)\n\n#     punctuation_pattern = r\"[^\\w\\s]\"\n\n#     # Add <SOS> and <EOS> tokens to each sentence and remove punctuation\n#     processed_sentences = [f\"{re.sub(punctuation_pattern, '', sentence).strip()}\" for sentence in sentences]\n    \n#     # Join sentences back into a single string\n#     return \" \".join(processed_sentences)\n\n# # Update the vocabulary building function to include this preprocessing\n# def build_vocabulary(dataset_dict, vocab_size=50, num_samples=None):\n#     # Initialize a counter for word frequencies\n#     word_counter = Counter()\n\n#     # Use tqdm to add a progress bar for the iteration\n#     data = dataset_dict['train']['text']\n    \n#     # Randomly sample num_samples if specified\n#     if num_samples:\n#         data = random.sample(data, num_samples)\n\n#     # Tokenize and clean text using the tokenizer and update word frequencies\n#     for text in tqdm(data, desc=\"Building vocabulary\", unit=\"text\"):\n#         processed_text = preprocess_text(text.lower())\n#         tokens = [token.replace('Ġ', '') for token in tokenizer.tokenize(processed_text)]\n#         word_counter.update(tokens)\n\n#     # Get the most common tokens and create a vocabulary dictionary\n#     vocab_dict = {word: idx for idx, (word, _) in enumerate(word_counter.most_common(vocab_size))}\n    \n#     # Convert the vocabulary dictionary to a DatasetDict\n#     vocab_dataset = DatasetDict({\n#         'train': Dataset.from_dict({'word': list(vocab_dict.keys()), 'index': list(vocab_dict.values())}),\n#         'validation': dataset_dict['validation']\n#     })\n#     return vocab_dataset\n\n# # Example usage\n# vocab_dataset = build_vocabulary(dataset, vocab_size=9996)\n# print(vocab_dataset['train'][:10])  # Print the first 10 tokens from the vocabulary dataset","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the Vocabulary","metadata":{}},{"cell_type":"code","source":"# vocab_dataset.save_to_disk('/kaggle/working/vocab_dict_v2')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the Vocabulary","metadata":{}},{"cell_type":"code","source":"loaded_vocab_dataset = DatasetDict.load_from_disk(config['VOCAB_DIRNAME'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_vocab = loaded_vocab_dataset['train']['word']\nnew_vocab_size = len(custom_vocab)\nprint(new_vocab_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_vocab_dict = {word: idx for idx, word in enumerate(custom_vocab)}\nif \"[UNK]\" not in custom_vocab_dict:\n    print(\"Adding [UNK] token to the vocabulary.\")\n    custom_vocab_dict[\"[UNK]\"] = len(custom_vocab_dict)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split the dataset","metadata":{}},{"cell_type":"code","source":"used_dataset_size = 100000","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sampled_dataset = dataset['train'].train_test_split(train_size=0.8, test_size=0.2)\ntrain_dataset, val_dataset = sampled_dataset['train'].select(range(int(0.8 * used_dataset_size))), sampled_dataset['test'].select(range(int(0.2 * used_dataset_size)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model and tokenizer","metadata":{}},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"# class Transformer21MFinalSingleLayer(nn.Module):\n#     def __init__(self, vocab_size=50258, d_model=192, nhead=4, num_encoder_layers=1, num_decoder_layers=1, dim_feedforward=768, max_len=1000, device=\"cpu\"):\n#         super(Transformer21MFinalSingleLayer, self).__init__()\n\n#         self.device = device\n        \n#         # Embedding layer\n#         self.embedding = nn.Embedding(vocab_size, d_model).to(self.device)\n        \n#         # Positional encoding\n#         self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, d_model)).to(self.device)  # Assuming max length of 1000\n\n#         # Transformer Encoder and Decoder layers\n#         encoder_layer = TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=0.1)\n#         decoder_layer = TransformerDecoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=0.1)\n        \n#         self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=num_encoder_layers).to(self.device)\n#         self.transformer_decoder = TransformerDecoder(decoder_layer, num_layers=num_decoder_layers).to(self.device)\n        \n#         # Output linear layer\n#         self.fc_out = nn.Linear(d_model, vocab_size).to(self.device)\n        \n#         self.logits = None  # Store logits as an attribute\n\n#     def forward(self, src, past_key_values=None):\n#         # Move src to the correct device\n#         src = src.to(self.device)\n        \n#         # Create tgt as src shifted by one position\n#         tgt = src[:, 1:].to(self.device)\n#         src = src[:, :-1]\n\n#         # Embedding and positional encoding\n#         src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n#         tgt = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n\n#         # Pass through the encoder\n#         memory = self.transformer_encoder(src)\n\n#         # Pass through the decoder with past_key_values if provided\n#         output = self.transformer_decoder(tgt, memory, memory_key_padding_mask=None, \n#                                    tgt_key_padding_mask=None, \n#                                    memory_mask=None, \n#                                    tgt_mask=None)\n\n#         # Output layer to vocab logits\n#         self.logits = self.fc_out(output)\n\n#         # Ensure logits are on the correct device\n#         self.logits = self.logits.to(self.device)\n\n#         return CausalLMOutputWithPast(\n#             loss=None, \n#             logits=self.logits, \n#         )\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"# Create a tokenizer from scratch with custom vocab\ntokenizer = Tokenizer(models.WordLevel(vocab=custom_vocab_dict, unk_token=\"[UNK]\"))\nbase_tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-1Layer-21M\")\n# Set up pre-tokenizer, normalizer, and decoder (as used in most tokenizers)\ntokenizer.normalizer = normalizers.Lowercase()\ntokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\ntokenizer.decoder = decoders.WordPiece()\n\n# Save the tokenizer to a file\ntokenizer.save(\"custom_tokenizer.json\")\n\n# Load this tokenizer into a PreTrainedTokenizerFast\ncustom_tokenizer = PreTrainedTokenizerFast(\n    tokenizer_file=\"custom_tokenizer.json\",\n    model_max_length=base_tokenizer.model_max_length\n)\n\n# Add special tokens if needed\ncustom_tokenizer.add_special_tokens({'additional_special_tokens': ['<sos>', '<eos>']})\ncustom_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n# Save the custom tokenizer\ncustom_tokenizer.save_pretrained(\"custom_tokenizer\")\n\n# Reload and print the vocabulary size to confirm\ncustom_tokenizer = AutoTokenizer.from_pretrained(\"custom_tokenizer\")\nprint(f\"Custom tokenizer vocabulary size: {custom_tokenizer.vocab_size}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Tokenize the dataset","metadata":{}},{"cell_type":"code","source":"# Tokenization function for HuggingFace dataset\ndef tokenize_function(examples):\n    return custom_tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=config['BLOCK_SIZE'])\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the dataset","metadata":{}},{"cell_type":"code","source":"# Convert tokenized dataset to PyTorch tensors\ntrain_dataset.set_format(type='torch', columns=['input_ids'])\nval_dataset.set_format(type='torch', columns=['input_ids'])\n\ntrain_loader = data.DataLoader(train_dataset, batch_size=config['BATCH_SIZE'], shuffle=True)\nval_loader = data.DataLoader(val_dataset, batch_size=config['BATCH_SIZE'], shuffle=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(len(custom_tokenizer))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training the model","metadata":{}},{"cell_type":"markdown","source":"## Initializing the model","metadata":{}},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-1Layer-21M\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model = Transformer21MFinalSingleLayer(device=config['DEVICE'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.resize_token_embeddings(len(custom_tokenizer))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total Parameters: {total_params}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.to(config['DEVICE'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optimizer and Loss Function","metadata":{}},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=config['LR'])\nloss_fn = torch.nn.CrossEntropyLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation function for validation set","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_model(training_model: torch.nn.Module, val_loader: torch.utils.data.DataLoader):\n    training_model.eval()\n    losses = torch.zeros(config['EVAL_ITER'])\n    for k in range(config['EVAL_ITER']):\n        batch = next(iter(val_loader))  # Get the batch as a single value\n        s_val = batch['input_ids'].to(config['DEVICE'])  # Access 'input_ids' from the batch\n        t_val = s_val[:, 1:].clone()  # Shift for language model prediction\n        s_val = s_val[:, :-1]  # Remove last token from source\n        \n        # Forward pass through the model\n        val_output = training_model(s_val)\n        val_logits = val_output.logits  # Access logits from the model's output\n        \n        # Reshape logits and targets\n        val_logits = val_logits.view(s_val.size(0) * s_val.size(1), config['VOCAB_SIZE'])\n        t_val = t_val.view(s_val.size(0) * s_val.size(1))\n        \n        # Compute the loss\n        losses[k] = torch.nn.functional.cross_entropy(val_logits, t_val).item()\n    \n    training_model.train()\n    return losses.mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training function ","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, optimizer, config, loss_fn):\n    \"\"\"\n    Trains the model and logs the training and validation losses, with progress tracking using tqdm.\n    \"\"\"\n\n    best_val_loss = float('inf')\n    patience_counter = 0\n    \n    try:\n        for epoch in range(config['EPOCHS']):\n            model.train()\n            epoch_loss = 0\n\n            epoch_progress = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{config['EPOCHS']}: \", leave=False)\n            \n            for b_idx, batch in enumerate(epoch_progress):\n                sources = batch['input_ids'].to(config['DEVICE'])\n                targets = sources[:, 1:].clone().to(config['DEVICE'])  # Shift for language model prediction\n                sources = sources[:, :-1]  # Remove last token from source\n                logits = model(sources)  # Access logits from the model output\n                \n                # Get the actual batch size and sequence length\n                batch_size = sources.size(0)\n                seq_length = sources.size(1)\n                \n                # Reshape logits and targets\n                logits = logits.logits.view(batch_size * seq_length, config['VOCAB_SIZE'])\n                targets = targets.view(batch_size * seq_length)\n                \n                loss = loss_fn(logits, targets)\n                wandb.log({\"loss\": loss.item()})\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                epoch_loss += loss.item()\n                avg_loss = epoch_loss / (b_idx + 1)\n                epoch_progress.set_postfix(training_loss=avg_loss)\n\n            avg_epoch_loss = epoch_loss / len(train_loader)\n            print(f\"Epoch {epoch+1}/{config['EPOCHS']} completed with average training loss: {avg_epoch_loss}\")\n            \n            val_loss = eval_model(model, val_loader)\n            print(f\"Validation loss after {epoch+1} epochs: {val_loss}\")\n            wandb.log({\"val_loss\": val_loss})\n\n            if val_loss < best_val_loss:\n                best_val_loss = val_loss\n                patience_counter = 0\n                print(f\"New best validation loss: {val_loss}.\")\n            else:\n                patience_counter += 1\n                print(f\"No improvement in validation loss. Patience counter: {patience_counter}\")\n                \n            if patience_counter >= config['PATIENCE']:\n                print(\"Early stopping triggered.\")\n                break\n            \n    except KeyboardInterrupt:\n        print(\"Training interrupted.\")\n    print(\"Training completed.\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Running the training loop","metadata":{}},{"cell_type":"code","source":"if not load_model:\n    train_model(model, train_loader, val_loader, optimizer, config, loss_fn)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the model","metadata":{}},{"cell_type":"code","source":"model_req_path = config['WORKING_DIR']+'/model'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not load_model:\n    if not os.path.exists(model_req_path):\n        os.mkdir(model_req_path)\n        \n    torch.save({'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict()\n                }, model_req_path+'/'+config['MODEL_NAME']+'.pt')\n\n    print(\"Model saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the model","metadata":{}},{"cell_type":"code","source":"if load_model and os.path.exists(config['LOAD_MODELPATH']):\n    checkpoint = torch.load(config['LOAD_MODELPATH'], weights_only=True)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    print(\"Loaded the model!\")\nelif not os.path.exists(config['LOAD_MODELPATH']):\n    print(\"Model directory not found! Please check the path.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Testing the model","metadata":{}},{"cell_type":"markdown","source":"## Functions for generating and scoring the outputs","metadata":{}},{"cell_type":"code","source":"def prepare_input(text, tokenizer, device, block_size=128):\n    # Tokenize and encode the input text\n    inputs = tokenizer(\n        text, return_tensors=\"pt\", padding=\"max_length\",\n        truncation=True, max_length=block_size\n    )\n    # Move input tensors to the appropriate device\n    return {key: val.to(device) for key, val in inputs.items()}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_text(model, tokenizer, input_text, config):\n    # Prepare the input\n    inputs = prepare_input(input_text, tokenizer, config[\"DEVICE\"], config[\"BLOCK_SIZE\"])\n    \n    # Generate text\n    output_ids = model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=config['MAX_OUT_TOKENS'],  # max tokens to generate\n        pad_token_id=tokenizer.pad_token_id,  # Ensure proper padding\n        eos_token_id=tokenizer.eos_token_id,  # End generation on EOS\n        do_sample=True,  # Enable sampling for variety in output\n        temperature=0.7,  # Adjust temperature for randomness in sampling\n        top_k=50  # Limit to top-k tokens to avoid unlikely predictions\n    )\n    \n    # Decode the generated IDs to text\n    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    return generated_text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_text_gemini(generated_text):\n    # Use the generative model directly for evaluation\n    model2 = gemini_ai.GenerativeModel(\"gemini-1.5-flash\")\n    \n    # Generate content and get the response\n    response = model2.generate_content(generated_text, request_options=RequestOptions(retry=retry.Retry(initial=10, multiplier=2, maximum=60, timeout=300)))\n    \n    # Check if response contains valid content\n    if response.candidates:\n        eval_response = response.candidates[0].content.parts[0].text  # Get the text of the first candidate\n        return eval_response\n    else:\n        print(\"No valid candidates in the response. Check the generated text or API settings.\")\n        return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Lists of input prompts (generated by ChatGPT)","metadata":{}},{"cell_type":"code","source":"input_texts_list = [\n    \"In a bustling city filled with secrets, a shadow loomed.\",\n    \"High in the mountains, a lone traveler braved the storm.\",\n    \"Beneath the waves, in a hidden underwater kingdom, life thrived.\",\n    \"It was a quiet night until the distant howls broke the silence.\",\n    \"In a world where dragons flew free, danger was never far.\",\n    \"On a small farm, a young girl discovered a mysterious egg.\",\n    \"In a town where everyone whispered, the new stranger caused a stir.\",\n    \"Under the light of the full moon, something magical began to stir.\",\n    \"In a time of peace, a hidden evil began to rise.\",\n    \"Amidst the golden sands of the desert, a lost caravan wandered.\",\n    \"In the heart of the enchanted forest, a hidden village thrived.\",\n    \"Aboard a ship sailing unknown seas, the crew faced a peculiar sight.\",\n    \"In a world where animals spoke, a young boy sought adventure.\",\n    \"Deep within the icy tundra, an ancient secret lay buried.\",\n    \"On a stormy night, a stranger knocked at the castle door.\",\n    \"In a village plagued by mysteries, a young detective took charge.\",\n    \"Across the galaxy, explorers marveled at a new world.\",\n    \"Underneath the quiet streets, a hidden society had formed.\",\n    \"Long ago, a powerful wizard disappeared without a trace.\",\n    \"At the edge of the world, a brave crew faced the unknown.\",\n    \"In a city that never slept, two souls crossed paths unexpectedly.\",\n    \"In a school for magical creatures, new students arrived.\",\n    \"Hidden in the clouds, a floating kingdom kept its secrets.\",\n    \"Far in the distant future, humanity encountered its first alien.\",\n    \"Under a blanket of stars, two friends made a promise.\",\n    \"In a library of forgotten books, a mysterious journal appeared.\",\n    \"Aboard a train that never stopped, secrets unraveled slowly.\",\n    \"Amid a sea of stars, a lone spaceship drifted in silence.\",\n    \"In a kingdom of snow and ice, a prophecy was foretold.\",\n    \"Deep in the jungle, explorers discovered a glowing stone.\",\n    \"In a village under a curse, a hero was born.\",\n    \"At the dawn of time, the first humans encountered magic.\",\n    \"On a distant moon, an outpost signaled for help.\",\n    \"In a quiet town, every night brought new mysteries.\",\n    \"Beneath the great pyramids, an ancient secret was uncovered.\",\n    \"In a world without color, a single red flower bloomed.\",\n    \"On a ship lost at sea, whispers of an island spread.\",\n    \"In the middle of nowhere, a door to another world appeared.\",\n    \"In a castle of mirrors, reflections began to act strangely.\",\n    \"On the edge of a cliff, a young prince made a fateful decision.\",\n    \"In a forest where time stood still, a visitor arrived.\",\n    \"In a kingdom ruled by animals, a lion declared his rule.\",\n    \"In a world where wishes came true, a girl wished for more time.\",\n    \"In a school where shadows came alive, mysteries abounded.\",\n    \"In a library that seemed endless, a strange book was found.\",\n    \"On a small island, villagers began to notice odd happenings.\",\n    \"In the great desert, a treasure was hidden for centuries.\",\n    \"In a city beneath the earth, a new ruler emerged.\",\n    \"In a world divided by seasons, an eternal summer began.\",\n    \"At the top of a mountain, a temple held the key to truth.\",\n    \"In a town where time rewound each day, mysteries deepened.\",\n    \"On a snowy peak, two climbers discovered an ancient statue.\",\n    \"In a mansion where paintings moved, a mystery unraveled.\",\n    \"At the crossroads of realms, two adventurers met.\",\n    \"In a school hidden in the woods, every student had a secret.\",\n    \"In a town of endless rain, hope was a rare sight.\",\n    \"On a train bound for nowhere, strange passengers arrived.\",\n    \"In a forest where trees whispered, a path emerged.\",\n    \"In a world where stars guided destiny, a comet foretold change.\",\n    \"In a lonely tower, a forgotten sorceress waited.\",\n    \"At the edge of a lake, the reflection showed another world.\",\n    \"In a world beneath the clouds, legends of the sky spread.\",\n    \"In a kingdom of night, a lone warrior sought dawn.\",\n    \"In a garden of eternal flowers, time stood still.\",\n    \"On an island that disappeared each night, a story began.\",\n    \"In a city that glittered like gold, shadows lurked.\",\n    \"In a land where dreams came alive, a nightmare was born.\",\n    \"On the longest night, a hero's journey began.\",\n    \"In a kingdom lost to time, an old legend resurfaced.\",\n    \"In a forest cloaked in fog, paths led nowhere.\",\n    \"In a small shop, a mysterious item granted wishes.\",\n    \"On the day the sun didn't rise, fear spread.\",\n    \"In a town with no maps, wanderers were welcome.\",\n    \"Under a sky of falling stars, two souls met.\",\n    \"In a house with endless rooms, a mystery unraveled.\",\n    \"In a town where no one aged, secrets were kept.\",\n    \"In the middle of the ocean, a floating castle appeared.\",\n    \"At the heart of the desert, a lone tree bloomed.\",\n    \"In a world ruled by music, silence was feared.\",\n    \"On the night of the festival, a strange guest arrived.\",\n    \"In a land where darkness ruled, a light began to shine.\",\n    \"In a city where everyone wore masks, truths hid.\",\n    \"In a world of whispers, silence was a power.\",\n    \"On the eve of battle, a hero was forged.\",\n    \"In a castle of glass, a kingdom looked on.\",\n    \"In the kingdom of echoes, a voice was heard.\",\n    \"In a forest where dreams came true, nightmares hid.\",\n    \"In a town with endless winters, a new day dawned.\",\n    \"In a realm where seasons changed daily, stories grew.\",\n    \"Under the gaze of ancient gods, mortals lived.\",\n    \"In a world frozen in time, a clock began to tick.\",\n    \"In a library of the lost, an old tale was read.\",\n    \"In a meadow under starlight, two friends found magic.\",\n    \"In a city where clocks ran backward, futures changed.\",\n    \"In a kingdom ruled by children, a new game began.\",\n    \"In a land where the moon never rose, stars told tales.\",\n    \"In the far north, where the aurora danced, legends lived.\",\n    \"At the edge of eternity, two lovers met.\",\n    \"In a village of music, silence brought fear.\",\n    \"In a world where memories could be traded, one boy remembered.\"\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prompts for scoring the output by Gemini","metadata":{}},{"cell_type":"code","source":"step_1_static = (\n    \"The following exercise, the student is given the beginning of a story. The student needs to complete it into a full story. \"\n    \"The exercise tests the student’s language abilities and creativity. The symbol *** marks the separator between the \"\n    \"prescribed beginning and the student’s completion: \"\n)\n\nstep_2 = (\n    \"Please provide your general assessment about the part written by the student (the one after the *** symbol). \"\n    \"Only give the ratings without description and overall could be omitted\"\n    \"Do not give explainations for the ratings\"\n    \"Give them in one single line, separated by semi-colon\"\n    \"Is it grammatically correct? Is it consistent with the beginning of the story? Pay special attention to whether the \"\n    \"student manages to complete the sentence which is split in the middle by the separator ***.\"\n)\n\nstep_3 = (\n    \"Now, grade the student’s completion in terms of grammar, creativity, consistency with the story’s beginning and \"\n    \"whether the plot makes sense. Moreover, please provide your best guess of what the age of the student might be, \"\n    \"as reflected from the completion. Choose from possible age groups: A: 3 or under. B: 4-5. C: 6-7. D: 8-9. E: 10-12. F: 13-16. \"\n    \"e.g. Grammar: 8/10, Creativity: 7/10, Consistency: 7/10, Age group: E (10-12)\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Getting the scores","metadata":{}},{"cell_type":"code","source":"pattern = r\"Grammar: (\\d+)/10; Consistency: (\\d+)/10; Creativity: (\\d+)/10; Age group: (.+)\"\nscore_list = []\ncount = 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not load_df:\n    for input_text in input_texts_list:\n        output_text = generate_text(model, custom_tokenizer, input_text, config)\n        dynamic_part = f\"{input_text} Story begins here:***  {''.join(output_text)}. *** The story ends here\"\n        final_prompt = f\"{step_1_static}{dynamic_part}\\n{step_2}\\n{step_3}\"\n        gemini_generated_response = evaluate_text_gemini(final_prompt)\n        gemini_generated_response = gemini_generated_response.strip()\n        count += 1\n    \n        # print(f\"{input_text}; {gemini_generated_response}\")\n    \n        match = re.search(pattern, gemini_generated_response)\n        if match:\n            grammar, consistency, creativity, age_group = match.groups()\n            score_list.append([input_text, int(grammar), int(consistency), int(creativity), age_group])\n        else:\n            score_list.append([input_text, 0, 0, 0, \"DNF\"])\n        print(f\"Number of prompts appended: {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Putting them in Pandas dataframe","metadata":{}},{"cell_type":"code","source":"df = pd.DataFrame(score_list, columns=[\"Input Prompt\", \"Grammar\", \"Creativity\", \"Consistency\", \"Age Group\"])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Saving the Pandas Dataframe","metadata":{}},{"cell_type":"code","source":"result_req_path = config['WORKING_DIR']+'/result'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if not load_df:\n    if not os.path.exists(result_req_path):\n        os.mkdir(result_req_path)\n        \n    df.to_pickle(result_req_path+'/'+'rating_df_'+config['MODEL_NAME']+'.pkl')\n\n    print(\"Dataframe saved!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loading the Pandas Dataframe","metadata":{}},{"cell_type":"code","source":"if load_df and os.path.exists(config['DF_PATH']):\n    df = pd.read_pickle(config['DF_PATH'])\n    print(\"Loaded dataframe!\")\nelif not os.path.exists(config['DF_PATH']):\n    print(\"Result directory not found! Please check the path.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}